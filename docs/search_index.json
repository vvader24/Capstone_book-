[["index.html", "Psychological communities Chapter 1 Preface 1.1 Table of Contents", " Psychological communities Vinita Vader 2022-03-08 Chapter 1 Preface This bookdown was created to explore the idea of dimensionaliy reduction as applied to the research question - how can one develop meaningful (culture-relevant groups) in a given population. It consists of an initial assessment of all the dimensionality reduction algorithms for understanding culture relevant groupings dependent on personality and values Building this book has been an effort of love and perseverance towards the bigger goal of understanding the idea of Culture and if there is a way to quantify it. Much work needs to go into this before one could make conclusive arguments about which algorithm works the best. This book should be viewed as more of a literature review of several high dimensionality algorithms rather than a reference book of any sort. The ideas introduced here are still under development and should be considered with caution. I would like to thank Gerard Saucier, Sanjay Srivastava, Sara Weston and Ryan Light for the insightful discussions on culture and its quantifiable properties. I am grateful to Daniel Anderson for his generous help with the coding and development of this book. I have immense gratitude for Daniel for his impeccable teaching skills and never ending patience. I would also like to thank Elizabeth McNeily for her objective feedback on my crazy ideas. I thank Irina Vetchinkina, for tolerating my late night coding frenzies and one sided conversations with Github. I thank my brother, Nachiket, for supporting me and my partner Pranav for being there by my side. 1.1 Table of Contents Introduction {#intro} In this chapter I introduce the research question and its importance. I introduce the Personality and Values dataset that will be used for demonstration purposes in the book. I explain the organization of the book. Cluster analysis {#cluster} In this chapter I explore the algorithms that have been used to retain optimum number of clusters. Network Analysis {#network} I propose a network model of culture and an iterative process that can potentially be used for simulating real world conditions for group building. Principal Component Analysis {#pca} In this chapter I use PCA, a very common dimension reduction technique, and its application to the research question. "],["intro.html", "Chapter 2 Introduction 2.1 Theoretical Background 2.2 Personality Dataset 2.3 Values Dataset 2.4 Analysis 2.5 Data orientation", " Chapter 2 Introduction 2.1 Theoretical Background Most cross-cultural studies in psychology have a common theme consisting of the difference in levels of a sample statistics based on a sample characteristic feature. This feature is mostly the geographical location where the sample data has been collected. This geographical location is treated as an analogy to the cultural identity of an individual. Let’s try understanding this another way. Do the following statements convey the same meaning? “Self esteem in collectivistic cultures was found to be significantly lower than those in individualistic cultures” “Self esteem in Chinese and Indian participants was found to be significantly lower than those American and Canadian participants.” Although the concepts of collectivism and individualism have become synonymous with how we understand cultural differences it is problematic to equate the demographic variable of location of data collection to the psychological makeup of participants. Another way of approaching this issue is by taking a closer look at the definition of culture. A common definition involves a shared history, language or ways of thinking which mark the characteristic way of living for people. Although one could point at the the likelihood of developing this phenomena in a population if people share a close proximity, one should also consider the growing spread of and access to information from far off lands and countries which influence our everyday life. The increased consumption of online content in forms of movies, series or any form of media is instrumental in building mindsets which could potentially indicate differences despite the similarity in demographic aspects of the people. Moreover we live in an age where it is likely that people experience acculturation without having to travel as against our ancestors who traveled miles to even hear people speaking a different language. One of the most important criticisms against using countries or ethnicities as synonymous to culture is the within group variance that often exceeds the between group variance in such data sets (baskerville2003hofstede?). This not only speaks to the underlying heterogeneity in the population’s psychological composition but also indicates that there is a need to question the assumption of homogeneity beyond the political boundaries of a nation. This brings us to the question of what is then a good method to identify groups in a data set. Or rather what is a good data set to answer the question of how many similar-minded groups can be identified in a population. There are two possibilities for this question in my opinion. One, Personality data set and two, Values data set. Let’s unpack each of theses. To begin with personality, it is essentially how people tend to describe themselves on variables that are representative of descriptions being used in their respective language. By definition, personality consists of enduring patters of thoughts, emotions, behaviors and motivations in a population. This is therefore an appropriate place to begin with in understanding culture. Assuming that the scale is developed and standardized in the language familiar to the population ( emic approach), it can be argued that the data is rich in reflecting thinking patterns of the people in the sample. The Distributive Model of Culture [rodseth1998distributive] specifies that we all carry varying degrees of cultural information with us in the society. The descriptive nature of personality and its close relation to culture makes it a potential candidate for unearthing the culture-relevant groups in the data. The current workflow will use the IPIP 25-item data set for understanding high dimensionality structure of culture relevant groups. Values on the other hand hold patterns of thinking and guiding principles that people tend to report through their understanding of societal sanctions. These social sanctions are developed in the context of ideas ranging from freedom, feminism, nationalism, education, patriarchy, harm,n money, etc. Values therefore become a foreground for tapping into cultural mindsets of individuals. The present workflow will also use a Values data set to demonstrate how meaningful groups can be identified. The main reason for working with the Values data set for this study is the need for a large data set in order to carry out simulations that we will talk more about. The World Values Survey is a body of work that has been collecting data on Values from several geographical locations across the globe. It’s sophisticated compendium of codebooks and organization of the collected data lends itself as a good starting ground for a study such as this one. Let’s take a closer look at this data set. 2.2 Personality Dataset Self report personality measures could serve as projections of similar mindsets and cultural similarities between participants in the study. For the purpose of demonstration we will use the bfi data from the psych package. This data is based on the 25-item IPIP scale by Goldberg (1999), collected in the Spring of 2010. It consists of over 2800 data points. It measures the five dimensions of personality, namely - Conscientiousness, Extraversion, Neuroticism, Agreeableness and Openness to experience. We will also be asssessing two types of the original data. One, the original raw data and two, the ipsatized version of the data - this will involve mean centering the data for every participant. 2.3 Values Dataset The World Values Survey (WVS) is being collected since 1981 as an effort towards enabling policy makers, governments, researchers and organizations to make sound decisions about developing and forming a better society. For the purpose of the present study, we will be using the Wave 6 of the World Values Survey. The data for this survey was collected between 2010 and 2014. 2.4 Analysis The overall intention of this work is to build ideas towards quantifying culture and building similar minded groups in a given data. In more statistical terms we can call this the Dimensionality reduction problem. The overall premise of this statistical method lies in reorganizing the data in a manner that helps us to understand the best possible value of the number of dimensions that can be used to represent the data. This helps to bring a mathematical structure to the analysis of the data for the given project. We are essentially trying to understand how many dimensions ( k&lt;p ) can be used to represent the data at hand. Several methods have been developed in the past to address the dimensionality problem. Cluster analysis Principal Component Analysis Network Analysis There are other dimensionality algorithms that could be seen as potentially important models in handling the dimensioanlity problem. Following are some of these examples and my reflections about these methods. t-distributed stochastic neighbor embedding (t-SNE) This algorithm uses a hyperparamater called “perplexity” that needs to be specified in order to carrying out dimensionality reduction. In essence perplexity is equivalent to specifying the number of k in k-means clustering. There is currently no method (that I know of) that can identify the optimal number of dimensions to be retained. Uniform manifold approximation and projection (UMAP) UMAP certainly has an advantage over t-SNE with its ability to preserve both local and global structure of the data. It also has computational advantages over t-SNE. Yet again there is no way of determining what an optimal number of dimensions is. One potential option could be using cross validations but this requires further explorations in terms of n_neighbor specifications. Autoencoders Autoencoder, a relatively novel technique used for dimensionality reduction, is trained over number of iterations with the use of gradient descent, and it aims at minimising the mean squared error. The neural network basis to this algorithm lays the key emphasis on the “bottleneck” hidden layer where the input layer information is further compressed. However, there are no guidelines to choose the size of this bottleneck layer. Potential of heat-diffusion for affinity-based trajectory embedding (PHATE) PHATE can also capture both the global and the local structure of the data. This algorithm lends itself to visualizing 2 or 3 dimensional data. Hence the appropriateness of this algorithm for high dimensional structures is questionable. We will explore Cluster analysis 3 , Principal Component analysis 5 and Network analysis 4 in the following chapters. 2.5 Data orientation We will be using the following libraries for data cleaning and data visualization. library(tidyverse) library(reactable) Functions Functions for outputting data and workflow operations used in the global environment will be introduced in this sub section across chapters. This is to keep the reader informed about the codes and results in the data and a broader effort towards the reproducibility. Each function will have comments which will be indicative of tasks the function is doing. #Function 1 #Call and pring the value for a specific country cntry_n &lt;- function(country_code) { data_raw %&gt;% count(C_COW_ALPHA) %&gt;% filter(C_COW_ALPHA == {{country_code}}) } #Function 2 #ncol and nrow at the same time length_CnR &lt;- function(data){ n_col &lt;- ncol(data) n_row &lt;- nrow(data) return(tibble(n_col, n_row)) } 2.5.1 Values There are columns in the data. We will be retaining variables which consist of self-report data scaling from 1 to 5 or 1 to 7. We will retain data for three countries for this analysis. Three countries considered to be least similar on aspects such as language, social structures and values will be retained. Let’s look at the sample sizes for each country. data_raw %&gt;% count(C_COW_ALPHA) %&gt;% arrange(desc(n)) %&gt;% reactable( defaultColDef = colDef( cell = function(value) format(value, nsmall = 1), align = &quot;center&quot;, minWidth = 70, headerStyle = list(background = &quot;#f7f7f8&quot;) ), columns = list( C_COW_ALPHA = colDef(name = &quot;Country Code&quot;), n = colDef(name = &quot;Sample size (n)&quot;) ), bordered = TRUE, highlight = TRUE ) Three countries for this analysis include - The United States of America (USA) ( n = 2232), India (IND) ( n = 4078) and Nigeria (NIG) ( n = 1759) We will retain data for these three countries with some data cleaning. We will remove data of those participants who have 20% or more missing values. data_val &lt;- data_raw %&gt;% filter(C_COW_ALPHA %in% c(&quot;USA&quot;,&quot;IND&quot;,&quot;NIG&quot;)) %&gt;% rename(&quot;country&quot; = C_COW_ALPHA) %&gt;% dplyr::select(country, V4:V9, V12:V22, V45:V56, V70:V79, V95:V146, -V144, V153:V160J, V192:V216, V228A:V228K) %&gt;% #remove cols which have all NA&#39;s janitor::remove_empty(which = &quot;cols&quot;) %&gt;% #remove cols which have 20% or more missing data (NA) purrr::discard(~sum(is.na(.x))/length(.x)* 100 &gt;=20) %&gt;% #impute the missing values with mutate_all(~ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)) %&gt;% remove_rownames() Here is a the same data transposed, where rows are variables and columns are participants. #Transpose data data_val_t &lt;- data_val %&gt;% mutate(ind = paste0(&quot;p&quot;, 1:nrow(data_val))) %&gt;% dplyr::select(ind, everything(), -country) %&gt;% t %&gt;% as.data.frame %&gt;% janitor::row_to_names(1) %&gt;% mutate_all(funs(as.numeric(as.character(.)))) 2.5.2 Personality Let’s take a look at the personality data set from the psych package. This data consists of a sample size of 2800. To the extent of my knowledge this data was collected in the US but the details need to be looked into. A1:A5 - items measuring Agreeableness C1:C5 - items measuring Conscientiousness E1:E5 - items measuring Extraversion N1:N5 - items measuring Neuroticism O1:O5 - items measuring Openness to experience library(psych) data_pty &lt;- bfi %&gt;% dplyr::select(A1:O5) data_pty %&gt;% reactable( defaultColDef = colDef( cell = function(value) format(value, nsmall = 1), align = &quot;center&quot;, minWidth = 70, headerStyle = list(background = &quot;#f7f7f8&quot;) ), bordered = TRUE, highlight = TRUE ) data_pty_t &lt;- bfi %&gt;% dplyr::select(A1:O5) %&gt;% t %&gt;% data.frame %&gt;% janitor::clean_names() data_pty_t %&gt;% reactable( defaultColDef = colDef( cell = function(value) format(value, nsmall = 1), align = &quot;center&quot;, minWidth = 70, headerStyle = list(background = &quot;#f7f7f8&quot;) ), bordered = TRUE, highlight = TRUE ) 2.5.3 Note on Data transformation The future versions of this study will explore the ipsatized version of the data. Ipsatization refers to the process of mean centering the data for every participant. It involves replacing every data point (\\(x_i\\)) for a participant (\\(x\\)) with the mean deviation of the data point (\\(x_i - \\bar{x}\\)). The ipsatize function from the multicon package can be used for this purpose. "],["cluster.html", "Chapter 3 Cluster analysis 3.1 Data preparation 3.2 K-means Clustering", " Chapter 3 Cluster analysis 3.1 Data preparation For the purpose of demonstration we will be using a subsample from the larger data set. set.seed(133) n = 200 val_sub &lt;- data_val %&gt;% drop_na() %&gt;% group_by(country) %&gt;% sample_n(n) %&gt;% ungroup %&gt;% dplyr::select(-country) #libraries for this chapter library(cluster) # clustering algorithms library(factoextra) # clustering algorithms &amp; visualization The data pre-processing involves two main steps: - remove variables with near zero variance - scale the data (Z-scores with mean as the center, and sd for spread); one could potentially make an argument for using mean average deviation for estimating the spread of the data. val_sub &lt;- val_sub[,-caret::nearZeroVar(val_sub)] #remove columns with near zero variance - we have already removed the columns with near zero variance so we move to the next step. val_sub&lt;- val_sub %&gt;% scale #Let&#39;s take a look at the data df &lt;- val_sub %&gt;% data.frame() df %&gt;% mutate_if(is.numeric, ~round(., 2)) %&gt;% reactable() 3.2 K-means Clustering This type of Clustering involves minimizing the within cluster variation. Hartigan and Wong(1979) provided the following algorithm for K-means Clustering \\[W(C_k) = \\sum_{x_1 \\in C_k}(x_i-μ_k)^2\\] where \\(x_i\\) is a data point in cluster \\(C_k\\) and \\(μ_k\\) is the mean of all values in cluster \\(C_k\\).To put it simply, Clustering is an optimization algorithm which and k-means is one variant which tries to build coherent clusters that minimize within cluster dissimilarity. 3.2.1 How to specify the number of clusters? One of the important questions across dimension reduction techniques involves how many dimensions are the most optimal to describe the data at hand. This is essentially what we intend to do when we want to identify the groups in the data given the features. In our case the features involve the use variables in the World Value Survey. Method WSS: The within cluster dispersion of data, also know as the error measure, is likely to drop more prominently after some clustering value. This creates a “elbow” like visual on the plot which is utilized to indicate the optimal number of clusters to model the data. fviz_nbclust(val_sub, kmeans, method = &quot;wss&quot;, k.max = 50, nboot = 200) + theme_minimal() + ggtitle(&quot;Method: WSS [Elbow method] &quot;)+ geom_vline(xintercept = 5, linetype = 2) Method: Silhoutte: This method determines the appropriateness of a data point in a cluster. In other words, it computes how similar an observation is to a given cluster as compared to neighboring clusters. It computes average silhoutte of observations for varying k values. the k configuration is dependent on the values assigned to the observations across clusters, if these values are high the k or the number of clusters is seen as the appropriate value for structuring the data. fviz_nbclust(val_sub, kmeans, method = &quot;silhouette&quot;)+ theme_minimal() + ggtitle(&quot;Method: Silhouette&quot;) Method: Gap stat: This method computes pooled within cluster sum of squares and compares it to a null model of a single component. fviz_nbclust(val_sub, kmeans, method = &quot;gap_stat&quot;) ## Clustering k = 1,2,..., K.max (= 10): .. done ## Bootstrapping, b = 1,2,..., B (= 100) [one &quot;.&quot; per sample]: ## .................................................. 50 ## .................................................. 100 Sum of square : This method relies on minimizing the within cluster variance and increasing between cluster variance. #Function to run this algorithm on pull_vals &lt;- function(n_center){ n_center &lt;- {{n_center}} withinness &lt;- mean(kmeans(val_sub, center = {{n_center}})$withinss) betweenness &lt;- kmeans(val_sub, center = {{n_center}})$betweenss l = list(n_center, withinness, betweenness) names(l) &lt;- c(&quot;n_center&quot;, &quot;withinness&quot;, &quot;betweenness&quot;) return(l) } ssc &lt;- map_df(1:40, pull_vals) plot_ssc &lt;- ssc %&gt;% gather(., key = &quot;measurement&quot;, value = value, -n_center) plot_ssc%&gt;% ggplot(aes(x=n_center, y=log10(value), fill = measurement)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;, alpha = .8) + labs(x = &quot;Number of Clusters&quot;, y = &quot;Log10 Total Sum of Squares&quot;, title = &quot;Cluster Model Comparison&quot;) + scale_x_discrete(name = &quot;Number of Clusters&quot;, limits = paste0(1:40)) + geom_hline(aes(yintercept = 4.5), linetype = 3, size = .2, alpha = .5)+ theme_minimal() As you can see in the plot above, the WSS drop as the number of clusters increase. It keeps dropping even when the cluster size is 40. The graph indicates that the between cluster variance does not increase by much after k = 35. We can consider this as an approximate of the value of optimal k for the data. Method: Bayesian Inference Criterion [BIC] library(mclust) d_clust &lt;- Mclust(as.matrix(val_sub), G=1:15, modelNames = mclust.options(&quot;emModelNames&quot;)) plot(d_clust) Method: Average of 30 indices The NbClust package in R contains up to 30 indices (for example, Hartigan (1975),Ratowsky and Lance(1978), etc. ) for determining the number of appropriate clusters. #Check with Talapas library(NbClust) # res.nbclust &lt;- NbClust(val_sub, distance = &quot;euclidean&quot;, min.nc = 2, max.nc = 9, method = &quot;complete&quot;, index =&quot;all&quot;) # # factoextra::fviz_nbclust(res.nbclust) + # ggtitle(&quot;NbClust&#39;s optimal number of clusters&quot;) + theme_minimal() res.nbclust_15 &lt;- NbClust(data, distance = &quot;euclidean&quot;, min.nc = 2, max.nc = 15,method = &quot;complete&quot;, index =&quot;all&quot;) factoextra::fviz_nbclust(res.nbclust) + res.nbclust_15 &lt;- NbClust(val_sub, distance = &quot;euclidean&quot;, min.nc = 2, max.nc = 15,method = &quot;complete&quot;, index =&quot;all&quot;) factoextra::fviz_nbclust(res.nbclust_15) + ggtitle(&quot;NbClust&#39;s optimal number of clusters&quot;) + theme_minimal() res.nbclust_15$Best.nc %&gt;% as_tibble() %&gt;% pivot_longer(cols = KL:SDbw, names_to = &quot;index&quot;) %&gt;% slice(1:26) %&gt;% count(value) %&gt;% ggplot(aes(x = factor(value), y = n))+ geom_col(fill = &quot;#F08080&quot;, alpha = .8)+ theme_minimal() + labs(title = &quot;NbClust&#39;s optimal number of clusters&quot;, x = &quot;Number of clusters k&quot;, y = &quot;Frequency among all indices&quot;) Hierarchical clustering We will set the distance method to euclidean. The linkage method we will be using here is the average linkage method. dist_mat &lt;- dist(val_sub, method = &#39;euclidean&#39;) hclust_avg &lt;- hclust(dist_mat, method = &#39;average&#39;) #plot(hclust_avg) - aborts session, data is too big Cluster tree library(clustree) tmp &lt;- NULL for (k in 1:11){ tmp[k] &lt;- kmeans(val_sub, k, nstart = 30) } df &lt;- data.frame(tmp) # add a prefix to the column names colnames(df) &lt;- seq(1:11) colnames(df) &lt;- paste0(&quot;k&quot;,colnames(df)) # get individual PCA df.pca &lt;- prcomp(df, center = TRUE, scale. = FALSE) ind.coord &lt;- df.pca$x ind.coord &lt;- ind.coord[,1:2] df &lt;- bind_cols(as.data.frame(df), as.data.frame(ind.coord)) clustree(df, prefix = &quot;k&quot;) df_subset &lt;- df %&gt;% select(1:8,12:13) clustree_overlay(df_subset, prefix = &quot;k&quot;, x_value = &quot;PC1&quot;, y_value = &quot;PC2&quot;) overlay_list &lt;- clustree_overlay(df_subset, prefix = &quot;k&quot;, x_value = &quot;PC1&quot;, y_value = &quot;PC2&quot;, plot_sides = TRUE) overlay_list$x_side overlay_list$y_side "],["network.html", "Chapter 4 Network Analysis 4.1 Community detection 4.2 What is Community detection? 4.3 Values data", " Chapter 4 Network Analysis One way of conceptualizing meaningful cultural groupings could involve using the idea of networks consisting of nodes and edges. A common way of representing graphs in social network analysis involves representing nodes as people where th edges indicate some mathematical relation between the nodes. As a thought experiment one could think of these edges as psychological distances between people. The valence (positive or negative) of these edges could indicate similarity (or dissimilarity) in the thought processes of people. The weights of these edges (indicated by the thickness of the edges in the graph) could indicate the magnitude or strength of the similarity (or dissimilarity) in the thought processes of people. Once we are able to conceptualize the edges as thinking patterns, we need a way to find meaningful clusters or communities in the graph that will indicate groups that are similar to each other in their ways of thinking. Identification of such groups will involve algorithms that seek to find commonalities in the nodes and group them according to other similar nodes. This process in network analysis can be defined as community detection. 4.1 Community detection The conceptual understanding of communities in the real world involves a level of consensus or agreement between members of the same community. For the purpose of demonstration we will be modeling psychological distances as personality variables and as Emancipative values. The community detection algorithms allow for mathematically modeling this idea of identifying clusters or communities in a network of participants which could be potential proxies of culture relevant groups in the data. #Libraries library(igraph) library(psych) 4.1.1 Personality data set.seed(229) #222 gave a sd = 0 library(psych) #Raw data data_bfi &lt;- bfi %&gt;% dplyr::select(-gender, -education, -age) %&gt;% drop_na() %&gt;% sample_n(200) %&gt;% # rowwise() %&gt;% # mutate(A = sum(c(A1, A2, A3, A4,A5)), # C = sum(c(C1, C2, C3, C4,C5)), # N = sum(c(N1, N2, N3, N4, N5)), # O = sum(c(O1, O2, O3, O4, O5)), # E = sum(c(E1, E2, E3, E4, E5))) %&gt;% # dplyr::select(A, C, N, O, E) %&gt;% t %&gt;% as.data.frame() %&gt;% janitor::clean_names() %&gt;% rename_all(funs(stringr::str_replace_all(., &#39;v&#39;, &#39;p&#39;))) %&gt;% cor #Ipsatized data data_bfi_ip &lt;- bfi %&gt;% select(-gender, -education, -age) %&gt;% drop_na() %&gt;% multicon::ipsatize() %&gt;% sample_n(200) %&gt;% t %&gt;% as.data.frame() %&gt;% janitor::clean_names() %&gt;% rename_all(funs(stringr::str_replace_all(., &#39;v&#39;, &#39;p&#39;))) %&gt;% cor #Plot the correlations - raw reshape2::melt(data_bfi) %&gt;% ggplot(aes(x=value)) + geom_density(fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;, alpha=0.8)+ labs(title = &quot;Distribution of correlations - Raw data&quot;, x = &quot;Range of correlation values&quot;)+ theme_ipsum() + theme(plot.title = element_text(size=9)) #Plot the correlations - ipsatized reshape2::melt(data_bfi_ip) %&gt;% ggplot(aes(x=value)) + geom_density(fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;, alpha=0.8)+ labs(title = &quot;Distribution of correlations - Ipsatized data&quot;, x = &quot;Range of correlation values&quot;)+ theme_ipsum() + theme(plot.title = element_text(size=9)) 4.2 What is Community detection? Community detection is the process of identifying similar nodes in a network. In order to find these similar nodes or communities we will be employing different algorithms based on Exploratory Graph analysis (EGA). EGA is a network psychometrics framework that intends to find clusters or communities within network models. The first step is to apply the graphical least absolute shrinkage and selector operator (GLASSO) to an inverse covariance matrix. This results in a Gaussian graphical model (or a network) wherein the edges are partial correlations and the nodes are targets. These targets are variables/features when applied to latent variable modeling. These targets would be, in our case, people. The networks would thus involve understanding the communities of participants in the data holding all the other connections constant (since we use partial correlation graphs). After estimating this graph we can further apply community detection algorithms to find the right connections between participants. The underlying framework for every community detection (CD) algorithm is slightly different from each other. We will be exploring the CD algorithms in the igraph package in R. Before we move on the algorithms it is important to understand one concept that will potentially affect the generation of these communities. The concept of modularity(Newman, 2006) is crucial to understanding any CD algorthim. Modularity in effect is the degree to which communities in a network have larger number of connections within the community in contrast to lesser connections outside the community. \\[ Q = 1/2w \\Sigma_{ij}(w_{ij}- \\frac{w_iw_j}{2w}) \\delta(c_i,c_j) \\] \\(w_{ij}\\) = edge strength (e.g., correlation) between nodes \\(i\\) and \\(j\\) \\(w_i\\) and \\(w_j\\) = node strength (e.g., correlation) for nodes \\(i\\) and \\(j\\) \\(w\\) = summation of all edge weights in the network \\(c_i\\), \\(c_j\\) = community that node \\(i\\) and node \\(j\\) belong to \\(\\delta\\) takes values 1 (when nodes \\(i\\) and \\(j\\) belong to the same community) and 0 (when nodes \\(i\\) and \\(j\\) belong to different communities) is 1 if the nodes belong to the same community (i.e., \\(c_i = c_j\\) ) and 0 if otherwise. Edge Betweenness This algorithm is based on the measure of betweenness cetrality in network graphs. The edge betweenness scores are computed based on how many shortest paths pass through a given edge in the network. The edge betweenness CD algorithm relies on the fact that edges with high betweenness are likely to connect multiple groups as these will the only ways for different groups/communities in the network to stay connected. The edge with the highest betweenness value is removed, followed by recomputing of betweenness for the remaining edges. The new highest betweenness value is identified and removed followed by another round of recomputation. An optimal threshold is established using modularity. # edge betweenness set.seed(98) data_bfi_eb &lt;- bfi %&gt;% select(-gender, -education, -age) %&gt;% drop_na() %&gt;% sample_n(70) %&gt;% t %&gt;% as.data.frame() %&gt;% janitor::clean_names() %&gt;% rename_all(funs(stringr::str_replace_all(., &#39;v&#39;, &#39;p&#39;))) %&gt;% cor g &lt;- graph.adjacency(data_bfi_eb , mode=&quot;upper&quot;, weighted=TRUE, diag=FALSE) e &lt;- get.edgelist(g) df &lt;- as.data.frame(cbind(e,E(g)$weight)) df &lt;- graph_from_data_frame(df, directed = F) eb &lt;- edge.betweenness.community(df) plot(eb, df) Fast and Greedy This algorithm begins with considering every node as one community and uses heirarchical clustering to build communities. Each node is placed in a community in a way that maximizes modularity. The communities are collapsed into different groups once the modularity threshold has been reached (i.e. no significant improvement in modularity is observed). Due to the high speed of this algorithm it is often a preferred approach for quick approximations of communities in the data. g &lt;- graph.adjacency(data_bfi, mode=&quot;upper&quot;, weighted=TRUE, diag=FALSE) e &lt;- get.edgelist(g) df &lt;- as.data.frame(cbind(e,E(g)$weight)) df &lt;- graph_from_data_frame(df, directed = F) #Fast and greedy algorithm fg &lt;- fastgreedy.community(df); fg ## IGRAPH clustering fast greedy, groups: 2, mod: 5.8e-17 ## + groups: ## $`1` ## [1] &quot;p1&quot; &quot;p2&quot; &quot;p3&quot; &quot;p4&quot; &quot;p5&quot; &quot;p6&quot; ## [7] &quot;p7&quot; &quot;p8&quot; &quot;p9&quot; &quot;p10&quot; &quot;p11&quot; &quot;p12&quot; ## [13] &quot;p13&quot; &quot;p14&quot; &quot;p15&quot; &quot;p16&quot; &quot;p17&quot; &quot;p18&quot; ## [19] &quot;p19&quot; &quot;p20&quot; &quot;p21&quot; &quot;p22&quot; &quot;p23&quot; &quot;p24&quot; ## [25] &quot;p25&quot; &quot;p26&quot; &quot;p27&quot; &quot;p28&quot; &quot;p29&quot; &quot;p30&quot; ## [31] &quot;p31&quot; &quot;p32&quot; &quot;p33&quot; &quot;p34&quot; &quot;p35&quot; &quot;p36&quot; ## [37] &quot;p37&quot; &quot;p38&quot; &quot;p39&quot; &quot;p40&quot; &quot;p41&quot; &quot;p42&quot; ## [43] &quot;p43&quot; &quot;p44&quot; &quot;p45&quot; &quot;p46&quot; &quot;p47&quot; &quot;p48&quot; ## [49] &quot;p49&quot; &quot;p50&quot; &quot;p51&quot; &quot;p52&quot; &quot;p53&quot; &quot;p54&quot; ## + ... omitted several groups/vertices # membership(fg) # communities(fg) Louvian This is similar to the greedy algorithm described above. This algorithm also uses heirarchical clustering. It intends to identify heirarchical structures wherein it swaps nodes between communities to assess improvement in modularity. Once the modularity reaches a point where no improvement is observes, the communities are modeled as latent nodes and edge weights with other nodes within and outside the community are computed. This provides a heirarchical or a multi-level structure to the communities identified. The results of Lovian and Fast Greedy algorithms are therefore, likely to be similar. # Louvain lc &lt;- cluster_louvain(df);lc ## IGRAPH clustering multi level, groups: 1, mod: 0 ## + groups: ## $`1` ## [1] &quot;p1&quot; &quot;p2&quot; &quot;p3&quot; &quot;p4&quot; &quot;p5&quot; &quot;p6&quot; ## [7] &quot;p7&quot; &quot;p8&quot; &quot;p9&quot; &quot;p10&quot; &quot;p11&quot; &quot;p12&quot; ## [13] &quot;p13&quot; &quot;p14&quot; &quot;p15&quot; &quot;p16&quot; &quot;p17&quot; &quot;p18&quot; ## [19] &quot;p19&quot; &quot;p20&quot; &quot;p21&quot; &quot;p22&quot; &quot;p23&quot; &quot;p24&quot; ## [25] &quot;p25&quot; &quot;p26&quot; &quot;p27&quot; &quot;p28&quot; &quot;p29&quot; &quot;p30&quot; ## [31] &quot;p31&quot; &quot;p32&quot; &quot;p33&quot; &quot;p34&quot; &quot;p35&quot; &quot;p36&quot; ## [37] &quot;p37&quot; &quot;p38&quot; &quot;p39&quot; &quot;p40&quot; &quot;p41&quot; &quot;p42&quot; ## [43] &quot;p43&quot; &quot;p44&quot; &quot;p45&quot; &quot;p46&quot; &quot;p47&quot; &quot;p48&quot; ## [49] &quot;p49&quot; &quot;p50&quot; &quot;p51&quot; &quot;p52&quot; &quot;p53&quot; &quot;p54&quot; ## + ... omitted several groups/vertices # membership(lc) # communities(lc) # plot(lc, g) Walktrap This algorithm starts with computing a transition matrix \\(\\mathbf{T_{ij}}\\) in which each matrix element \\(p_{ij}\\) is the probability of one node \\(i\\) traversing to another node \\(j\\). \\[\\mathbf{T_{ij}} = \\left[\\begin{array} {rrr} p_{11} &amp; p_{12} &amp; p_{13} \\\\ p_{21} &amp; p_{22} &amp; p_{23} \\\\ p_{31} &amp; p_{32} &amp; p_{33} \\end{array}\\right]\\] In the matrix above, \\(p_{32}\\) is th probability that node 3 traverses to node 2 as determined by the node strengths of the two nodes. Ward’s agglomorative clustering approach is employed wherein nodes start off as a cluster of their own and then merge with adjacent clusters. The merging takes place in a way where the sum of squared distances between clusters are reduced. #Random Walk wk &lt;- walktrap.community(df); wk ## IGRAPH clustering walktrap, groups: 200, mod: 0 ## + groups: ## $`1` ## [1] &quot;p1&quot; ## ## $`2` ## [1] &quot;p2&quot; ## ## $`3` ## [1] &quot;p3&quot; ## ## $`4` ## + ... omitted several groups/vertices # membership(wk) # communities(wk) Infomap This is similar to the greedy walk algorithm except it converts the random walk information into a binary coding system. The partition of data into networks is carried out in a way that maximizes the information of random walks. # Infomap imc &lt;- cluster_infomap(df);imc ## IGRAPH clustering infomap, groups: 1, mod: 0 ## + groups: ## $`1` ## [1] &quot;p1&quot; &quot;p2&quot; &quot;p3&quot; &quot;p4&quot; &quot;p5&quot; &quot;p6&quot; ## [7] &quot;p7&quot; &quot;p8&quot; &quot;p9&quot; &quot;p10&quot; &quot;p11&quot; &quot;p12&quot; ## [13] &quot;p13&quot; &quot;p14&quot; &quot;p15&quot; &quot;p16&quot; &quot;p17&quot; &quot;p18&quot; ## [19] &quot;p19&quot; &quot;p20&quot; &quot;p21&quot; &quot;p22&quot; &quot;p23&quot; &quot;p24&quot; ## [25] &quot;p25&quot; &quot;p26&quot; &quot;p27&quot; &quot;p28&quot; &quot;p29&quot; &quot;p30&quot; ## [31] &quot;p31&quot; &quot;p32&quot; &quot;p33&quot; &quot;p34&quot; &quot;p35&quot; &quot;p36&quot; ## [37] &quot;p37&quot; &quot;p38&quot; &quot;p39&quot; &quot;p40&quot; &quot;p41&quot; &quot;p42&quot; ## [43] &quot;p43&quot; &quot;p44&quot; &quot;p45&quot; &quot;p46&quot; &quot;p47&quot; &quot;p48&quot; ## [49] &quot;p49&quot; &quot;p50&quot; &quot;p51&quot; &quot;p52&quot; &quot;p53&quot; &quot;p54&quot; ## + ... omitted several groups/vertices # membership(imc) #communities(imc) # plot(fg, g) Eigen vector This involves computing an eigenvector for the modularity matrix and splitting the network into communities in order to improve modularity. A stopping condition is specified to avoid tight communities to be formed. Due to eigenvector computations, this algorithm does not work well with degerate graphs. # Eigen vector community eg &lt;- leading.eigenvector.community(df);eg ## IGRAPH clustering leading eigenvector, groups: 1, mod: 0 ## + groups: ## $`1` ## [1] &quot;p1&quot; &quot;p2&quot; &quot;p3&quot; &quot;p4&quot; &quot;p5&quot; &quot;p6&quot; ## [7] &quot;p7&quot; &quot;p8&quot; &quot;p9&quot; &quot;p10&quot; &quot;p11&quot; &quot;p12&quot; ## [13] &quot;p13&quot; &quot;p14&quot; &quot;p15&quot; &quot;p16&quot; &quot;p17&quot; &quot;p18&quot; ## [19] &quot;p19&quot; &quot;p20&quot; &quot;p21&quot; &quot;p22&quot; &quot;p23&quot; &quot;p24&quot; ## [25] &quot;p25&quot; &quot;p26&quot; &quot;p27&quot; &quot;p28&quot; &quot;p29&quot; &quot;p30&quot; ## [31] &quot;p31&quot; &quot;p32&quot; &quot;p33&quot; &quot;p34&quot; &quot;p35&quot; &quot;p36&quot; ## [37] &quot;p37&quot; &quot;p38&quot; &quot;p39&quot; &quot;p40&quot; &quot;p41&quot; &quot;p42&quot; ## [43] &quot;p43&quot; &quot;p44&quot; &quot;p45&quot; &quot;p46&quot; &quot;p47&quot; &quot;p48&quot; ## [49] &quot;p49&quot; &quot;p50&quot; &quot;p51&quot; &quot;p52&quot; &quot;p53&quot; &quot;p54&quot; ## + ... omitted several groups/vertices 4.3 Values data set.seed(133) val_data &lt;- data_val %&gt;% group_by(country) %&gt;% sample_n(50) %&gt;% ungroup %&gt;% dplyr::select(-country) %&gt;% t %&gt;% cor #Plot the correlations reshape2::melt(val_data) %&gt;% ggplot(aes(x=value)) + geom_density(fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;, alpha=0.8)+ labs(title = &quot;Distribution of correlations - Values Raw data&quot;, x = &quot;Range of correlation values&quot;)+ theme_ipsum() + theme(plot.title = element_text(size=9)) g &lt;- graph.adjacency(val_data, mode=&quot;upper&quot;, weighted=TRUE, diag=FALSE) e &lt;- get.edgelist(g) df &lt;- as.data.frame(cbind(e,E(g)$weight)) df &lt;- graph_from_data_frame(df, directed = F) # edge betweenness #eb &lt;- edge.betweenness.community(df);eb #plot(eb, df) # Louvain lc &lt;- cluster_louvain(df);lc ## IGRAPH clustering multi level, groups: 1, mod: 0 ## + groups: ## $`1` ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; ## [8] &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; ## [15] &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; ## [22] &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; ## [29] &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; ## [36] &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; ## [43] &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; ## [50] &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; &quot;56&quot; ## [57] &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; &quot;63&quot; ## + ... omitted several groups/vertices # membership(lc) # communities(lc) # plot(lc, g) # Infomap imc &lt;- cluster_infomap(df);imc ## IGRAPH clustering infomap, groups: 1, mod: 0 ## + groups: ## $`1` ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; ## [8] &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; ## [15] &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; ## [22] &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; ## [29] &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; ## [36] &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; ## [43] &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; ## [50] &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; &quot;56&quot; ## [57] &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; &quot;63&quot; ## + ... omitted several groups/vertices # membership(imc) # communities(imc) # plot(imc, g) #Fast and greedy algorithm fg &lt;- fastgreedy.community(df); fg ## IGRAPH clustering fast greedy, groups: 2, mod: 1.9e-17 ## + groups: ## $`1` ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; ## [8] &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; ## [15] &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; ## [22] &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; ## [29] &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; ## [36] &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; ## [43] &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; ## [50] &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; &quot;56&quot; ## [57] &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; &quot;63&quot; ## + ... omitted several groups/vertices # membership(fg) # communities(fg) # Eigen vector community eg &lt;- leading.eigenvector.community(df);eg ## IGRAPH clustering leading eigenvector, groups: 1, mod: 0 ## + groups: ## $`1` ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; ## [8] &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; ## [15] &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; ## [22] &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; ## [29] &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; ## [36] &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; ## [43] &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; ## [50] &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; &quot;56&quot; ## [57] &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; &quot;63&quot; ## + ... omitted several groups/vertices # communities(eg) #Random Walk wk &lt;- walktrap.community(df); wk ## IGRAPH clustering walktrap, groups: 150, mod: 0 ## + groups: ## $`1` ## [1] &quot;1&quot; ## ## $`2` ## [1] &quot;2&quot; ## ## $`3` ## [1] &quot;3&quot; ## ## $`4` ## + ... omitted several groups/vertices # membership(wk) # communities(wk) "],["pca.html", "Chapter 5 Principal Component analysis 5.1 Data Preparation 5.2 Parallel analysis (PA)", " Chapter 5 Principal Component analysis One of the most important algorithms used in dimensionality reduction is PCA. This method works very well with large datasets which makes it ideal for tasks such as this one. 5.1 Data Preparation We will be working with a sample size of 50 from the Values data. This will be utilized for demonstrating the code. The problem of positive definite matrices although has not been completely solved at this point, the demonstrations are quite helpful. set.seed(133) n = 50 data_PA &lt;- data_val %&gt;% drop_na() %&gt;% group_by(country) %&gt;% sample_n(n) %&gt;% ungroup %&gt;% dplyr::select(-country) %&gt;% t %&gt;% cor data_PA %&gt;% reshape2::melt(value.name = &quot;cor&quot;) %&gt;% ggplot(aes(x=cor)) + geom_density(fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;, alpha=0.8)+ labs(title = &quot;Distribution of correlations - Values Raw data, n= 50&quot;, x = &quot;Range of correlation values&quot;)+ theme_ipsum() + theme(plot.title = element_text(size=9)) 5.2 Parallel analysis (PA) PA is considered to be one of the most trusted algorithms to determine the number of factors to be retained in a dataset. It has been proven to be more accurate than several other statistical methods such as the elbow method using the scree plot, Kaiser rule, or Velicer’s MAP test. Traditional PA compares eigenvalues (\\(\\lambda_1\\)…\\(\\lambda_k\\)) generated by the data at hand with averages of simulated eigenvalues (\\(\\bar{\\lambda}_1\\)…\\(\\bar{\\lambda}_k\\)). Correlation matrices consisting of the same dimensions are randomly generated a specific number of times and averages of all eigenvalues simulated are retained. The mean simulated eigenvalues are compared with the eigenvalues obtained in the data. When the \\(\\lambda_k\\) &gt; \\(\\bar{\\lambda}_k\\), you retain the eigenvalue. The number of eigenvalues retained is considered as the most optimum number for identifying the dimensionality of the data. #Function for building Parallel analysis plots PA_graph &lt;- function(data){ library(paran) library(scales) PA_allData &lt;- paran({{data}}, iterations = 1000, centile = 0, quietly = TRUE, status = TRUE, all = TRUE, cfa = TRUE, graph = FALSE) PA_df &lt;- tibble(PA_allData$AdjEv, PA_allData$Ev, PA_allData$RndEv, PA_allData$Bias) PA_df &lt;- PA_df %&gt;% janitor::clean_names() %&gt;% tibble::rownames_to_column() %&gt;% mutate(rowname= as.numeric(rowname)) %&gt;% rename(&quot;pc&quot; = rowname, &quot;adj_ev&quot; = pa_all_data_adj_ev, &quot;ev&quot; = pa_all_data_ev, &quot;rnd_ev&quot; = pa_all_data_rnd_ev, &quot;bias&quot; = pa_all_data_bias) %&gt;% pivot_longer(cols = c(adj_ev, ev, bias)) PA_df %&gt;% ggplot(aes(x = pc, y = value, color = name))+ geom_point(size = 1.5)+ geom_line()+ scale_x_continuous(name=&#39;Factor Number&#39;, breaks= pretty_breaks()) + scale_y_continuous(name=&#39;Eigenvalue&#39;, breaks= pretty_breaks()) + labs(legend_title = &quot;Type of values&quot;)+ scale_color_manual(values=c(&quot;#69b3a2&quot;, &quot;#85C1E9&quot;, &quot;#F5B041&quot;))+ geom_vline(xintercept = PA_allData$Retained, linetype = &#39;dashed&#39;, color = &quot;#566573&quot;)+ #geom_text(aes(x=10, label=&quot;Retain 5 components&quot;, y=40), position = position_dodge(width=0.9), colour=&quot;#566573&quot;, angle=90, size=2)+ guides(color = guide_legend(title=&quot;Iteration type&quot;))+ theme_minimal() + theme(plot.title = element_text(size=9)) } PA_graph(data_PA) ## ## Using eigendecomposition of correlation matrix. ## Computing: 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% "],["personality-data-1.html", "Chapter 6 Personality data 6.1 Bayesian Parallel Analysis", " Chapter 6 Personality data One of the important things to recognize is that I need to remove rows that have zero variance. These participants have responded to the questionnaire with only one consistent option and this could cause issues with the correlation matrix estimation. This could also lead to issues is eigendecomposition of the matrix. set.seed(900) PA_pty &lt;- bfi %&gt;% dplyr::select(-gender, -education, -age) %&gt;% drop_na() %&gt;% #slice(-drop_row) %&gt;% sample_n(200) %&gt;% t %&gt;% cor() PA_graph(PA_pty) ## ## Using eigendecomposition of correlation matrix. ## Computing: 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 6.1 Bayesian Parallel Analysis This is a relatively new method to compute Parallel analysis. We will be using a code from a recent paper to carry out this analysis. 6.1.1 Personality data library(matrixcalc) #Personality data set.seed(18) bfi %&gt;% dplyr::select(-gender,-education,-age) %&gt;% drop_na %&gt;% sample_n(20) %&gt;% t %&gt;% cor %&gt;% bayes.pa 6.1.2 Values data set.seed(453) samp = 45 data_val %&gt;% group_by(country) %&gt;% sample_n(samp) %&gt;% ungroup %&gt;% dplyr::select(-country, -V12, -V13, -V14, -V15, -V16, -V17, -V18, -V19, -V20, -V21, -V22) %&gt;% drop_na() %&gt;% mutate(id = paste0(&quot;p&quot;, 1:(samp*3))) %&gt;% ungroup %&gt;% t %&gt;% data.frame %&gt;% janitor::clean_names() %&gt;% mutate_all(., as.integer) %&gt;% map_dfr(.,jitter) %&gt;% drop_na %&gt;% cor %&gt;% #is.positive.definite() bayes.pa "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
