
```{r include=FALSE}
source("common.R") 
```

# Cluster analysis 

## Data preparation 
The data pre-processing involves two main steps:
 - remove variables with near zero variance
 - scale the data (Z-scores with mean as the center, and sd for spread); one could potentially make an argument for using mean average deviation for estimating the spread of the data. 
```{r}
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
data <- data[,-caret::nearZeroVar(data)] #remove columns with near zero variance
data <- data %>% dplyr::select(-country) %>% scale
```

## K-means Clustering 
This type of Clustering involves minimizing the within cluster variation. Hartigan and Wong(1979) provided the following algorithm for K-means Clustering 

$$W(C_k) =  \sum_{x_1 \in C_k}(x_i-μ_k)^2$$
where $x_i$ is a data point in cluster $C_k$ and $μ_k$ is the mean of all values in cluster $C_k$.To put it simply, _Clustering_ is an optimization algorithm which and _k-means_ is one variant which tries to build coherent clusters that minimize within cluster dissimilarity.  

### How to specify the number of clusters?
\n
One of the important questions across dimension reduction techniques involves how many dimensions are the most optimal to describe the data at hand. This is essentially what we intend to do when we want to identify the groups in the data given the features. In our case the features involve the use variables in the World Value Survey. 


- **Method WSS**: The within cluster dispersion of data, also know as the _error measure_, is likely to drop more prominently after some clustering value. This creates a "elbow" like visual on the plot which is utilized to indicate the optimal number of clusters to model the data. 

```{r}
 fviz_nbclust(data, kmeans, method = "wss", k.max = 50, nboot = 200) + theme_minimal() + ggtitle("Method: WSS [Elbow method] ")+ geom_vline(xintercept = 5, linetype = 2)

```

- **Method: Silhoutte**
```{r}
fviz_nbclust(data, kmeans, method = "silhouette")+ theme_minimal() + ggtitle("Method: Silhouette")
```

- **Method: Gap stat**
```{r eval=FALSE}
fviz_nbclust(data, kmeans, method = "gap_stat")
```

```{r}
knitr::include_graphics(rep("images/gap_stat.png"))
```

- **Sum of square **: This method relies on minimizing the within cluster variance and increasing between cluster variance. 
```{r}

pull_vals <-  function(n_center){
  
  n_center <- {{n_center}}
  withinness <- mean(kmeans(data, center = {{n_center}})$withinss)
  betweenness <-  kmeans(data, center = {{n_center}})$betweenss
  
   l = list(n_center, withinness, betweenness) 
   names(l) <- c("n_center", "withinness", "betweenness")
      return(l)
  }

ssc <- map_df(1:30, pull_vals)

plot_ssc <- ssc %>%
  gather(., key = "measurement", value = value, -n_center) 

plot_ssc%>% 
    ggplot(aes(x=n_center, y=log10(value), fill = measurement)) +
       geom_bar(stat = "identity", position = "dodge", alpha = .8) +     
       labs(x = "Number of Clusters", 
            y = "Log10 Total Sum of Squares", 
            title = "Cluster Model Comparison") +
       scale_x_discrete(name = "Number of Clusters", limits = paste0(1:30)) +
  geom_hline(aes(yintercept = 4.5), linetype = 3, size = .2, alpha = .5)+
       theme_minimal()
```

As you can see in the plot above, the WSS drop as the number of clusters increase. It keeps dropping even when the cluster size is 30. This is a larger kmeans value. 

- **Method: Bayesian Inference Criterion [BIC]**
```{r eval=FALSE}
library(mclust)
d_clust <- Mclust(as.matrix(data), G=1:15, 
                  modelNames = mclust.options("emModelNames"))
d_clust$BIC
plot(d_clust)
```

```{r}
knitr::include_graphics(rep("images/BIC.png"))
```

- **Method: Average of 30 indices **
The  `NbClust` package in R contains up to 30 indices (for example, Hartigan (1975),Ratowsky and Lance(1978), etc. ) for determining the number of appropriate clusters.
```{r eval=FALSE}
library(NbClust)
res.nbclust <- NbClust(data, distance = "euclidean", min.nc = 2, max.nc = 9, method = "complete", index ="all")


res.nbclust_15 <- NbClust(data, distance = "euclidean", min.nc = 2, max.nc = 15,method = "complete", index ="all")

factoextra::fviz_nbclust(res.nbclust) + 
   ggtitle("NbClust's optimal number of clusters") + theme_minimal()
```

